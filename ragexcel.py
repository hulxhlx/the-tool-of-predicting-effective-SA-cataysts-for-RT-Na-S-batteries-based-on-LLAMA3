# -*- coding: utf-8 -*-
"""
Created on Tue Sep 10 15:55:34 2024

@author: Lingxiang
"""

import ollama
import time
import os
import json
import numpy as np
from numpy.linalg import norm
import pandas as pd


# open a file and return paragraphs
def parse_file(filename):
    with open(filename, encoding="utf-8-sig") as f:
        paragraphs = []
        buffer = []
        for line in f.readlines():
            line = line.strip()
            if line:
                buffer.append(line)
            elif len(buffer):
                paragraphs.append((" ").join(buffer))
                buffer = []
        if len(buffer):
            paragraphs.append((" ").join(buffer))
        return paragraphs


def save_embeddings(filename, embeddings):
    # create dir if it doesn't exist
    if not os.path.exists("embeddings"):
        os.makedirs("embeddings")
    # dump embeddings to json
    with open(f"embeddings/{filename}.json", "w") as f:
        json.dump(embeddings, f)


def load_embeddings(filename):
    # check if file exists
    if not os.path.exists(f"embeddings/{filename}.json"):
        return False
    # load embeddings from json
    with open(f"embeddings/{filename}.json", "r") as f:
        return json.load(f)


def get_embeddings(filename, modelname, chunks):
    # check if embeddings are already saved
    if (embeddings := load_embeddings(filename)) is not False:
        return embeddings
    # get embeddings from ollama
    embeddings = [
        ollama.embeddings(model=modelname, prompt=chunk)["embedding"]
        for chunk in chunks
    ]
    # save embeddings
    save_embeddings(filename, embeddings)
    return embeddings


# find cosine similarity of every chunk to a given embedding
def find_most_similar(needle, haystack):
    needle_norm = norm(needle)
    similarity_scores = [
        np.dot(needle, item) / (needle_norm * norm(item)) for item in haystack
    ]
    return sorted(zip(similarity_scores, range(len(haystack))), reverse=True)



# 假设文件名为 "data.xlsx"，请根据实际的文件名进行修改
def write_paragraph(file_path):
    # 读取Excel文件
    try:
        df = pd.read_excel(file_path)
    except Exception as e:
        print(f"Error reading Excel file: {e}")
        return

    # 打印列名检查是否正确
    print("Columns in the Excel file:", df.columns)

    paragraphs = []
    
    # 检查列名是否存在
    if 'Column1' not in df.columns or 'Column2' not in df.columns:
        print("Error: 'Column1' or 'Column2' not found in the Excel file.")
        return

    # 遍历每一行并合并Column1和Column2
    for index, row in df.iterrows():
        paragraph = f"Title: {row['Column1']} Abstract: {row['Column2']}\n"  # 合并列并加上换行符
        paragraphs.append(paragraph)
    
    return paragraphs


def main():
    SYSTEM_PROMPT = """You are a helpful reading assistant who answers questions 
        based on snippets of text provided in context.The context below are the abstract and title of papers. Answer only using the context provided, 
        being as concise as possible. If you're unsure, just say that you don't know.These are the title and the abstract of the paper
        Context:
    """
    # open file
    filename = "output.txt"
    #paragraphs = parse_file(filename)
    file_path = "./dataset_240820.xlsx"
    
    paragraphs = write_paragraph(file_path)

    # embeddings = get_embeddings(filename, "llama3", paragraphs)
    # print(np.array(embeddings).shape)
    embeddings = np.load('240820.npy')
    
    prompt = input("what do you want to know? -> ")
    # strongly recommended that all embeddings are generated by the same model (don't mix and match)
    prompt_embedding = ollama.embeddings(model="llama3", prompt=prompt)["embedding"]
    # find most similar to each other
    most_similar_chunks = find_most_similar(prompt_embedding, embeddings)[:100]

     
    df = pd.DataFrame(columns=["Iteration", "Response","Title and Abstract:"])

    for i in range(100):
        # 调用ollama.chat生成响应
        response = ollama.chat(
            model="llama3",
            messages=[
                {
                    "role": "system",
                    "content": SYSTEM_PROMPT
                    + "\n".join(paragraphs[most_similar_chunks[i][1]]),
                },
                {"role": "user", "content": prompt},
            ],
        )

        # 获取生成的内容并将其与编号一起写入DataFrame
        response_content = response["message"]["content"]
        new_row = pd.DataFrame({
            "Iteration": [i+1], 
            "Response": [response_content], 
            "Title and Abstract": [paragraphs[most_similar_chunks[i][1]]]
        })
        df = pd.concat([df, new_row], ignore_index=True)
        
        

    # 将结果保存到Excel文件
    df.to_excel("output_responses.xlsx", index=False)
    print("Responses written to 'output_responses.xlsx'.")

if __name__ == "__main__":
    main()
